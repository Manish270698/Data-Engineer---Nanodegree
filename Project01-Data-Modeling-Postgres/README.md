## Sparkify Data Modeling using Postgres

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.


### Project Description:
Write an ETL pipeline that transfers data from given files in two local directories into the tables in Postgres using Python and SQL.


### Data
#### Song Dataset: 
Each file is in JSON format and contains metadata about a song and the artist of that song.

 Eg:
{"num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id": "SOMZWCG12A8C13C480", "title": "I Didn't Mean To", "duration": 218.93179, "year": 0}

#### Log dataset: 
This dataset consists of log files in JSON format generated by this event simulator based on the songs in the song dataset. These simulate activity logs from a music streaming app based on specified configurations.

Eg: 
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}

### Dataset Schema:
The schema used is star schema. This includes the following tables.

#### Fact Table
###### songplays - records in log data associated with song plays i.e. records with page NextSong
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables
**users** - users in the app
user_id, first_name, last_name, gender, level
**songs** - songs in music database
song_id, title, artist_id, year, duration
**artists** - artists in music database
artist_id, name, location, latitude, longitude
**time** - timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday


### Project structure

Files used on the project:

1. **data** folder nested at the home of the project, where all needed jsons reside.

2. **sql_queries.py** contains all your sql queries, and is imported into the files bellow.

3. **create_tables.py** drops and creates tables. You run this file to reset your tables before each time you run your ETL scripts.

4. **test.ipynb** displays the first few rows of each table to let you check your database.

5. **etl.ipynb** reads and processes a single file from song_data and log_data and loads the data into your tables. 

6. **etl.py** reads and processes files from song_data and log_data and loads them into your tables. 

7. **README.md** current file, provides discussion on my project.

### Break down of steps followed

1. Wrote DROP, CREATE and INSERT query statements in sql_queries.py

2. Run in terminal/console

 ```

python3 create_tables.py

```

3. Used test.ipynb Jupyter Notebook to interactively verify that all tables were created correctly.

4. Followed the instructions and completed etl.ipynb Notebook to create the blueprint of the pipeline to process and insert all data into the tables.

5. Once verified that base steps were correct by checking with test.ipynb, filled in etl.py program.

6. Run etl in terminal/console:

 ```

python3 etl.py

```

6. Verify results using test.ipynb notebook

### ETL pipeline (etl.py)

1. We start our program by connecting to the sparkify database.

2. We walk through the files under /data/song_data, and for each json file encountered we use the function called process_song_file :

    a. We load the file as a dataframe using a pandas function "read_json()".

    b. For each row in the dataframe we select the fields we are interested in:

        ```

        song_data = [song_id, title, artist_id, year, duration]

        ```

        ```

         artist_data = [artist_id, artist_name, artist_location, artist_longitude, artist_latitude]

        ```

    c. We insert this data into their respective tables.

6. We walk through the files under /data/log_date, and for each json file encountered we use the function called process_log_file

    a. We load our data as a dataframe same way as with songs data. 

    b. We select rows where page = 'NextSong' only

    c. We convert ts column where we have our start_time as timestamp in millisencs to datetime format. We obtain the parameters we need from this date (day, hour, week, etc), and insert everything into our time dimentional table.

    d. We store user data into our user table

    e. Finally we lookup song and artist id from their tables by song name, artist name and song duration that we have on our song play data. The query used is the following:

        ```

        song_select = ("""

            SELECT song_id, artists.artist_id

            FROM songs JOIN artists ON songs.artist_id = artists.artist_id

            WHERE songs.title = %s

            AND artists.name = %s

            AND songs.duration = %s

        """)

        ```

    f. The last step is inserting everything we need into our songplay fact table.